# Query Processing and Execution

The query processor is the collection of components that turns user queries and data modification commands into a sequence of database operations, and then executes those operations

**Basic Division**

Query compilation:
    - Parsing
    - Rewrite (changing SQL into relational algebra)
    - Physical Plan generation

Last two (rewrite and physical plan generation) are referred to as Query Optimization

It's the hard part of query processing. 

## Physical Query Plan operators

Physical query plans are built from operators

Often are just physical implementations of relational algebra operators

Also include physical tasks not in relational algebra, such as bringing a table from disk into main memory.

Called a table "scan."

### Scanning Tables

Simplest physical query plan involves reading the tuples of some relation R
- Variation involves a predicate, applied to each tuple 

Two approaches for table scans:
- System knows the blocks containing the tuples of R. Blocks are obtained one by one. Called a *table-scan*
- There's an index on R (possibly a sparse index). Use the index to find the right blocks. Called an *index-scan*

Using an index scan makes it easy to apply a predicate, not just obtain all the tuples of R. 

### Sorting while scanning tables

Reasons for wanting to sort the relation:
- Most obvious is an `ORDER BY` clause 
- Other reasons might include relational algebra expressions (grouping)

The operator *sort-scan* takes a relation R and a set of attributes on which to sort, and produces R in sorted order

Several approaches:
- If there's a B-Tree on R for the appropriate attributes, we can use that.
- If R will fit in main memory, we use a table-scan and sort in memory
- Otherwise, use a Multiway Merge Sort

### Cost Model for operations

We again focus on the number of disk I/Os that are needed for the operation. 

Additional assumption: the arguments for any operator are found on disk, but the result of the operation is left in main memory. 

Reason: many operations have their result written to disk. Even when the results are written to disk, their cost is only related to the size of the result. 

#### Other parameters for measuring cost

We need to measure the amount of memory an operation uses

Assume that memory is divided into buffers, which are the same size as disk blocks. 

Let M be the number of buffers available. Often this is the amount of memory on the system, though in practice it will be limited based on what other operations are going on. 

Another assumption that data is accessed one block at a time from disk. 

Three parameter families: B, T, V

B(R) or B, is the number of blocks needed to hold all the tuples of R. 
- If R is clustered (generally assume), B(R) implies that R uses only B blocks 
- Otherwise, the number could be much larger 

T(R) or T, is the number of tuples in R

For some relation R, with attribute a, V(R, a) is the number of distinct values of a appearing in R. 

### I/O Cost for Scan operators

If R is clustered, the number of disk I/Os for a table scan is B. 

If B < M (R fits into memory), a scan-sort also costs B. 

If R is not clustered, the costs are higher, potentially T. 

For an index scan, there is a cost to loading the index, but it's sufficiently smaller than B(R) that we can ignore it.

So an index scan of R costs B, regardless of whether R is clustered or not. 

(Simpler/cheaper still if we ony need part of R and can use the index to avoid loading all the tuples).

#### Iterators

We can implement many physical operators as an iterator, with the usual `open()`, `getNext()`, and `close()` methods. 

Depending on the operation, this can save on memory. 

Table scan for example now only needs one block at a time. But a sort-scan still has to do all of the work in the `open()` method: no tuple may be operated on via the `getNext()` until all the tuples have been read. 

## Algorithms for physical query operators

Three categories:
- Sorting-based methods
- Hash-based methods
- Index-based methods

We can also divide into three "degrees" of difficulty and cost:
- One pass algorithms: Data is read only once. Usually one argument must fit entirely in memory
- Two pass algorithm: we read all the data, do some work, write the result back to disk, read again
- Multipass Algorithms: no practical limit on the size of the data on which they operate

## One-Pass Algorithms

3 types:
- Tuple-at-a-time Unary operations 
- Full relation unary operations
- Full relation binary operations

### Tuple-at-a-time operations

Includes selection or projection, which have obvious algorithms:

We read the blocks of R one at a time into an input buffer, perform the operation, move the result to an output buffer. 

We don't consider the output buffer in terms of required space. 

These algorithms require only M >= 1

Disk I/O requirement depends on how R is provided. 

If R is on disk, the cost is the same as a table or index scan: B if R is clustered, T if R is not clustered.

(A relevent index might improve this). 

### One-pass Unary Full-relation operations

Apply to relations as a whole: duplicate elimination, grouping

#### Duplicate Elimination:

-Read each block of R
- For each tuple, we consider is this the first time we've seen it?
    - If yes: copy to output buffer
    - If no: don't copy 

We need to keep in memory one copy of every tuple.

One memory buffer holds the working block of R. 

The remaining M - 1 buffers may hold copies of previously examined tuples.

We choose an appropriate data structure: usually a hashtable or balanced binary search tree

Memory usage: B (d(R)) <= M 

In generaly, we can't know if we have sufficient memory without doing the computation. 

#### Grouping 

We have grouping attributes and aggregated attributes

We keep one main memory entry for each group with the values of grouping attributes and:
- Min/Max: we maintain the current min/max, replacing when needed
- Count: maintain the count, increment as needed 
- Sum: accumulated sum, adding as we go 
- Avg: we maintain both the sum and count, until all the tuples have been seen 

We can't produce any output until all the tuples have been seen. 

As with duplicate elimination, hashtable or balanced search tree for holding the groups in memory (using the grouping attributes as the search key)

The number of disk I/Os is B 

Memory usage is not related to B. Typically M < B 

Groups could potentially be longer than the tuples. 

**Note on nonclustered data** We typically assume R is clustered. If it's not the disk I/Os could be T instead of B.

However, if the input is the result of a previous operation, it will always be clustered. 

### One Pass Algorithms for Binary Data

Set operations, Products, and Joins

For the Set operations, we need to distinguish between the set and bag versions. 

#### Bag Union 

R Union S is simple: make one pass through both R and S.

Disk I/Os is B(R) + B(S)

M = 1

**All the other operations** require reading the smaller of the two (conventionally S) into memory, and building a suitable data structure. 

Approximate min(B(R), B(S)) <= M 

One buffer used for the blocks of the larger relation, the rest of memory is used for the tuples of the smaller

#### Set Union

- Read the smaller relation
    - Build a search structure whose key is the whole tuple 
- All tuples are copied to output
- Read the larger relation
    - For each tuple, if it exists in the search structure, ignore it. 
    - Otherwise, copy it to output

#### Set Intersection

Similar to Set union, except:
- We don't copy any of the tuples of the smaller relation to output
- If any tuple of the larger relation exists in the search structure, copy it to output

#### Set Difference:

Start in the same way: copy S to memory (in a search structure)

R - S: if a tuple of R isn't in the search structure, copy it to output

S - R: if a tuple of R is in the search structure, delete it. At at the end, copy to output the remaining tuples in the search structure

#### Bag Intersection:

We we read S into buffers, we give each unique tuple a count (no need to store multiple copies)

Cost is still B(S) <= M 

For each tuple in R, if it appears in the search structure with a positive count: copy to output, and decrement the count. 

#### Bag difference

Start the same way: read S and count occurences of tuples

S - R: For each tuple of R, decrement the count if it's in the search structure

Output the tuples that remain with a positive count. 

R - S: For each tuple t in R, if t doesn't exist in the search structure, or if the count is 0, output t. Otherwise decrement the count

#### Product

R x S

Read S into M-1 buffers

For each tuple of R, produce a pair with each tuple of s in memory. Copy the formed tuple to output. 

#### Natural Join:

Assume that R(XY) and S(YZ),

Read all tuples of S into memory. Build a main memory search structure using Y as the search key. M-1 blocks of memory 

For each tuple t in R, use the search structure to find the tuples of S that agree with t on Y. Create a joined tuple and copy to outpu.

Also takes B(R) + B(S) Disk I/Os.

Other joins are similar. 

### Nested Loops Joins

"One and a half" passes: one of the relations will be read only once, but the other will be read repeatedly.

Nested loops can be used for relations of any size. 

**Tuple-Based Nested Loop**

Compute R(XY) Join S(YZ)

- For each tuple s in S
    - for each r in R
        - construct tuple t from s and r 
        - if s.y == r.y
            - output t 

Worst case could be T(R)*T(S) Disk I/Os

An index would help if available

Another option is a block-based iterator:
- We organize access to both argument relations by blocks and
- Store as much of S as possible in memory

The first point reduces the number of reads we do for the tuples of R
The second point allows us to join each tuple of R with as much of S as possible, each time it's read.

``` 
open() {
    R.open()
    S.open()
    s := S.getNext()
}

getNext() {
    repeat:
        r := R.getNext()
        if (r == NotFound) {
            R.close()
            s := s.getNext()
            if (s == NotFound)
                Return NotFound 
            
            R.open()
            r := r.getNext()
    until (r and s join)
    return the join of r and s
}

close() {
    R.close()
    S.close()
}
```

We assume that B(S) <= B(R)

Now we assume that B(S) > M

Repeatedly load M-1  blocks of S into memory. Build a search structure with a key of Y (common attributes)

Go through all of R, finding the tuples that join

Simplied:

``` 
For each M-1 chunk of S:
    load into memory
    organize into a search structure
    For each block b of R
        read b into memory
        For each tuple t of b
            find tuples of S in memory that join
            output the join
```

#### Performance of Nested Loop Joins

Number of chunks in the outer loop s B(S)/(M-1)

Each iteration, we read M-1 blocks of S, and B(R)

B(S)*(M-1 + B(R))/(M-1)

Roughly: B(S)*B(R)/M 

Better options are available when both R and S are large.

When one is marginally bigger than memory, the cost isn't much greater than a one-pass join. 