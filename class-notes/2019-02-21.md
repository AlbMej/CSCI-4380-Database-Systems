# Secondary Storage Management

Database systems always involve secondary storage to store large amounts of data over time. 

## Memory Hierarchy

Multiple components for data storage
- Amount of data stored by different components varies by 7 (or more) orders of magnitude
- Speed of data access by different components varies by 7 (or more) orders of magnitude
- Cost of storage per byte differs by 3 orders of magnitude

**Cache** On-board memory (usually on the chip itself). Also includes level-2 cache on another chip

**Main Memory** RAM. All of the operations we consider a computer doing, in terms of work on information happens at this level.

Transfer to cache takes 10-100 nanoseconds 

**Secondary Storage** Disk. Often a spinning magnetic disk (increasingly SSD)

Time to transfer to main memory is around 10msec. 

Large amounts of data can be transferred at once, so speed of transfer is complex

**Tertiary Storage** Magnetic Tape, optical drives. 

Much longer retrieval times. Much longer persistence. Lower cost per byte. 

## Transfer of Data between levels

Data moves between adjacent levels. 

Data on disk is organized into *blocks* (4-64 kb). Entire blocks are transferred at once, to or from a contiguous section of main memory called *buffer*. 

An implication is that we can improve performance if data that is needed at the same time is stored on the same block.

## Volatile vs Non-volatile storage

Volatile storage "forgets" what is stored when the power is shut off. 

Much of the complexity in a DBMS comes from the fact that a change can't be considered final until it's written to non-volatile storage. 

## Virtual Memory

A system to increase the size of memory address space beyond what's physically available in RAM. 

It's feature of operating systems and is not typically applicable to DBMS

## Disks

For the purposes of this class, we'll focus on spinning magnetic drives, not solid state drives. 

Magnetic disks are still used for databases, though it's becoming less common. 

Some of the principles surrounding the organization of data on magnetics may apply at a more macro level to databases that are more broadly distributed. 

A magnetic disk consists of a *Disk Assembly* and a *head assembly*.

The disk assembly consists of one or more circular platters that rotate around a central spindle. 

The upper and lower surfaces are covered in a magnetic material on which bits are stored

The disk is organized into *tracks* which are concentric circles of a platter. 

The tracks at a given distance from the center across all surfaces, form a cylinder. 

Density of data is greater along a track than radially. 

Tracks are separated into *sectors* by gaps in the magnetic material. 

A sector is an indivisible unit, as far as reading and writing are concerned. 

Blocks are logical units of data and are stored on one or more sectors. 

The head assembly holds the disk heads. One head for each surface. The head reads data as the disk spins beneath it.

The head assembly moves as a single unit.

### Disk Controller

Disk drives are controlled by a disk controller, a small processor capable of:
1. Controlling the activator to move the head assembly 
2. Selecting a sector from all those in a given cylinder. It knows when the desired sector moves under a head.
3. Transferring bits between the desired sector and Main Memory
4. Buffering a track or more in local memory in anticipation of its use

## Disk access characteristics

Accessing a disk block requires 3 steps, each with an associated delay
1. Disk controller moves the head to the right cylinder. (Seek Time)
2. Controller waits until the right sector moves under the head (rotational latency)
3. All sectors (and gaps) pass under the under the head (transfer time)

The sum of seek time, rotational latency, and transfer time is the latency of the disk

Seek time is usually between 0 and 10 msec. 

Rotational Latency is between 0 and 10 msec. 5 msec average

Transfer times are usually sub-msec range 

Typical latency averages around 10 msec. 

##Improving Throughput

Disk latency isn't the only delay. Requests may have to wait, and *scheduling latency* becomes an issue. 

Worst case scenario, requests arrive faster than they can be filled, and scheduling latency becomes infinite. 

There are things we can do to improve throughput (number of disk accesses per second).
- Place blocks that are accessed together on the same cylinder
- Divide data among multiple smaller disks instead of one large one
- "Mirror" a disk (making a copy on another disk)
- Use a disk scheduling algorithm to adjust the order in which requests are filled
- Pre-fetch blocks into main memory

### I/O Model of computation 

We assume one processory, one disk controller, one disk.

Assume the database is too large to fit into main memory. 

The time taken to perform a disk access is much larger than the time spent manipulating data in main memory. Therefore, the number of block accesses is a good approximation of the time needed by an algorithm 

### Organizing data by cylinders

Seek Time represents about half of disk latency 

We can store data likely to be accessed together on the same cylinder. Then we can ignore seek time for all but the first block

If blocks are stored on consecutive sectors, rotational latency can be ignored. 

### Using Multiple Disks

We can improve performance by replacing one large disk with muliple smaller disks. This gives us multiple independent heads. 

No change in access time for any single request. 

*Striping* can speed up access to objects that occupy a large number of blocks 

Say we have relation R that takes up 6 blocks

``` 
Disk 1      Disk 2      Disk 3      Disk 4
-------------------------------------------
R1 R5       R2 R6       R3          R4
```

**Mirroring** disks involves making identical copies of the data and spreading them over multiple disks. 

Helps with relisiency (always have a backup)

For n disks, read time improves by a factor of n

Write time doesn't improve, as we still have to write to each disk

### Disk Scheduling

We don't have to fill requests in the order in which they were received.

Elevator Algorithms 

If a disk head is passing cylinders it knows contain data that must be accessed, it can stop and do it then. 

### Pre-fetching

Also call buffering (or Double-Buffering)

Some applications allow us to anticipate what data will be needed next. We can pre-fetch that data, allowing for better scheduling. 

## Disk Failures

Forms of disk failure:
- Intermittent failure: an attempt to read or write a sector is unsuccessful but subsequent attempts succeed. 
- Media Decay: it becomes impossible to read or write a sector, regardless of the number of attempts. 
- Write Failure: an attempt to write fails, but the previous data can't be retrieved. Often caused by a loss of power
- Disk Crash: entire disk becomes unusable, suddenly and permanently

### Checksums

An approach used to detect failure

Each sector has some bits that are set dependening on the values of the other bits in the sector

Not perfect, but if we use enough bits for the checksum, the probability of missing a failure is sufficiently low.

Simplest form of checksum is the parity bit. 

If there are an odd number of 1's in a sector, we say that it has *odd parity* and add a 1 parity bit.

If there are an even number of 1's in a sector, we say that it has *even parity* and add a 0 parity bit.

The number of 1's in a sector and its parity bit is always even.

``` 
Data        Parity
--------------------
10011011    1
10101100    0

```

A big drawback is that it only detects 1 bit failure. Multiple bit failures have a 50% chance of being undetected

We can use multiple parity bits for a sector

```
Data        
------------
1011 
0011   
1010 
0111
0101 
1000

Parity
-----------
1000

```

A massive failure will likely be detected. Chance it's missed is 1/2 for each parity bit. 

1 / 2^n for n parity bits

### Stable Storage Policies

Checksums detect errors but can't correct. 

We can use a "stable storage policy"

Essentially, we pair sector of the disk with a sector on another disk. 

Assume a sufficient number of parity bits to detect error 

By writing one copy first and waiting until the write is successful before attempting to write the other copy, we can't lose data.

## Recovery from Disk Crashes 

We use multiple disks to reduce the risk of data loss. Such schemes are called RAID: Redundant Arrays of Independent Disks.

The idea is that some disks hold data (data disks), Others hold information deteremined by the data (Redundant disks).

Because disks typically don't fail at the same time, we can use the redundant disks to recreate the data on a failed disk.

Simples scheme is Mirroring. **RAID level 1**.

We have a completely redundant disk. No data data loss unless there's a second failure while the first failure is being repaired.

RAID 1 is expensive (lots of redunant disks)

**RAID level 4** uses only 1 redundant disk, regardless of many data disks.

Assume identical disks. One disk holds the parity bits for all the others

``` 
Disk 1      Disk 2      Disk 3      Disk 4
-------------------------------------------
1011001     10110010    01010101    01001110
0100110     01001111    01101100

Data is on disks 1-3, parity is on disk 4
```

Reading is the same as without RAID 

Writing is more complicated, as the parity bits need to be updated as well. 

Naive approach: do the write, recalculate the parity bits

Better approach: take the modulo 2 sum of the old and new data, and change the parity bits where the modulo 2 sum is 1.

``` 
Disk 1      Disk 2      Disk 3      Disk 4
-------------------------------------------
10110010     10110001    01010101    01001100
01001100     01001111    01101100    01101111

Old: 10110010
New: 10110001
```

Recovery from disk failure: swap out the failed disk. Write the modulo 2 sum of all the other disks. 

``` 
Disk 1      Disk 2      Disk 3      Disk 4
-------------------------------------------
10           10110001    01010101    01001100
             01001111    01101100    01101111
```

One downside: having all the parity bits on one disk creates a bottleneck

**RAID level 5** spreads the parity bits over all the disks

``` 
Data:
1011 
0011   
1010 
0111
0101 
1000




Disk 1  Disk 2  Disk 3  Disk 4
0010*    1011    0011    1010
0111    1010*    0101    1000


```

## Multiple Disk Failures

Using error correcting codes called Hamming Codes, we can build systems that recover from multiple disk failures. 

**RAID level 6**

Basic Illustration

```
Disk    1   2   3   4   |   5   6   7
----------------------------------
        1   1   1   0   |   1   0   0
        1   1   0   1   |   0   1   0
        1   0   1   1   |   0   0   1

```

Every possible 2-digit column appears 

Columns for redudant disks have one 1

Columns for data disks have at least two 1's

Think of the set of disks with 1's in any given row as a RAID 4 array

``` 
1       2       3       4       5       6       7
1011    0011    1010    0111    0010    1111    0010
   
 


```